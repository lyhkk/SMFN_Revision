原来模型的问题：
    4.Test中会出现GT与LR都没有的锯齿 -> 训练次数太少，模型？
    5.ws_psnr和ws_ssim感觉虚高，而且psnr有方法上的冗余，太多种方案，却没有本质区别
    6.如何使用hr_texture和lr_texture还需要探索
    
现在要做的事情：
    1.找数据集
    2.高效的训练，更好的训练结果
    3.轻量化
    
最新进度：
    1.有多少layer(29+16199372)
    3.diffusion model可不可以加在后面，这样是不是可以取消融合模块
    4.是否每一个帧都需要三个级别的特征 ×
    5.可不可以记忆前面帧的三个级别特征，减少参数 ×
    6.残差学习感觉参数量会增大，是否它的贡献与占用资源成正比
    7.能不能用更先进的算法平替一些模块
    8.现在的结果是那些冗余多了呢
    9.dbpn感觉效率不高
    10.hr_texture, lr_texture怎么用
    
the total number of parameters: 16199372

train_set和validation_set的dataset类返回值不一样，没法通过随机分割产生train和val数据集，因为在之后的dataloader迭代时，dataloader返回的元组的内容不同

硬件问题：
    1.电脑性能支持DataLoader的num_workers=4吗
    2.gpu数量的设置

目标：
1.明白每个模块的作用
2.找到轻量化的方法
2.5 制作数据集
3.了解这两三年的新方法，找到提升模型能力的方法
4.界面

改进方案：
1.可变形卷积模块：可变形卷积在保持较好的效果的同时，增加了一定的计算量。因此，可以考虑使用更轻量的卷积操作替代可变形卷积，如深度可分离卷积等。

2.Self-attention模块：自注意力模块通常需要较高的计算和存储成本，可以考虑使用较轻量级的注意力模块来代替，如SE模块等。

3.PCD对齐模块：PCD对齐模块通常用于对齐高分辨率和低分辨率的特征图，在轻量化模型时可以考虑使用其他的对齐方式，如双线性插值等。

4.ResNet模块：ResNet是一个深度网络，它的层数较多，因此在轻量化时可以考虑减少其深度，或使用更轻量级的网络结构。

5.对偶网络：对偶网络通常用于提高超分辨率效果，但同时也增加了计算成本。在轻量化时，可以考虑减少对偶网络的深度或使用其他的低成本模型来代替。

做了什么:
   1.ws_psnr:峰值信噪比